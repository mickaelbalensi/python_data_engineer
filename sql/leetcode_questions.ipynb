{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **601. Human Traffic of Stadium**\n",
    "https://leetcode.com/problems/human-traffic-of-stadium/description/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+\n",
      "| id|visit_date|people|\n",
      "+---+----------+------+\n",
      "|  1|2017-01-01|    10|\n",
      "|  2|2017-01-02|   109|\n",
      "|  3|2017-01-03|   150|\n",
      "|  4|2017-01-04|    99|\n",
      "|  5|2017-01-05|   145|\n",
      "|  6|2017-01-06|  1455|\n",
      "|  7|2017-01-07|   199|\n",
      "|  8|2017-01-09|   188|\n",
      "+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"CreateTable\").getOrCreate()\n",
    "\n",
    "# Define Schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"visit_date\", DateType(), False),\n",
    "    StructField(\"people\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# Sample Data\n",
    "data = [\n",
    "    (1, datetime.strptime(\"2017-01-01\", \"%Y-%m-%d\").date(), 10),\n",
    "    (2, datetime.strptime(\"2017-01-02\", \"%Y-%m-%d\").date(), 109),\n",
    "    (3, datetime.strptime(\"2017-01-03\", \"%Y-%m-%d\").date(), 150),\n",
    "    (4, datetime.strptime(\"2017-01-04\", \"%Y-%m-%d\").date(), 99),\n",
    "    (5, datetime.strptime(\"2017-01-05\", \"%Y-%m-%d\").date(), 145),\n",
    "    (6, datetime.strptime(\"2017-01-06\", \"%Y-%m-%d\").date(), 1455),\n",
    "    (7, datetime.strptime(\"2017-01-07\", \"%Y-%m-%d\").date(), 199),\n",
    "    (8, datetime.strptime(\"2017-01-09\", \"%Y-%m-%d\").date(), 188)\n",
    "]\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Register as Table\n",
    "df.createOrReplaceTempView(\"stadium\")\n",
    "\n",
    "# Verify Data\n",
    "spark.sql(\"SELECT * FROM stadium\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/16 11:07:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/16 11:07:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/16 11:07:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+----+\n",
      "| id|visit_date|people|diff|\n",
      "+---+----------+------+----+\n",
      "|  2|2017-01-02|   109|   1|\n",
      "|  3|2017-01-03|   150|   1|\n",
      "|  5|2017-01-05|   145|   2|\n",
      "|  6|2017-01-06|  1455|   2|\n",
      "|  7|2017-01-07|   199|   2|\n",
      "|  8|2017-01-09|   188|   2|\n",
      "+---+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_query = \"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW consecutive AS\n",
    "SELECT \n",
    "    *, \n",
    "    id - ROW_NUMBER() OVER (ORDER BY id) AS diff\n",
    "FROM stadium \n",
    "WHERE people >= 100\n",
    "\"\"\"\n",
    "spark.sql(filter_query)\n",
    "\n",
    "# Show Consecutive Data\n",
    "spark.sql(\"SELECT * FROM consecutive\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/16 11:11:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/16 11:11:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+----+---+\n",
      "| id|visit_date|people|diff|rnk|\n",
      "+---+----------+------+----+---+\n",
      "|  2|2017-01-02|   109|   1|  2|\n",
      "|  3|2017-01-03|   150|   1|  2|\n",
      "|  5|2017-01-05|   145|   2|  4|\n",
      "|  6|2017-01-06|  1455|   2|  4|\n",
      "|  7|2017-01-07|   199|   2|  4|\n",
      "|  8|2017-01-09|   188|   2|  4|\n",
      "+---+----------+------+----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/16 11:11:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/16 11:11:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "query3 = \"\"\"CREATE OR REPLACE TEMP VIEW rnk_view AS\n",
    "\n",
    "    SELECT *, COUNT(*) OVER (PARTITION BY diff) AS rnk\n",
    "    FROM consecutive\n",
    "\"\"\"\n",
    "df_with_rnk = spark.sql(query3)\n",
    "spark.sql(\"SELECT * FROM rnk_view\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/16 11:10:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/16 11:10:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/16 11:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/16 11:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+\n",
      "| id|visit_date|people|\n",
      "+---+----------+------+\n",
      "|  5|2017-01-05|   145|\n",
      "|  6|2017-01-06|  1455|\n",
      "|  7|2017-01-07|   199|\n",
      "|  8|2017-01-09|   188|\n",
      "+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query4 = \"\"\"\n",
    "SELECT id, visit_date, people  \n",
    "FROM rnk_view\n",
    "WHERE rnk >= 3\n",
    "ORDER BY id\n",
    "\"\"\"\n",
    "df_result = spark.sql(query4)\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **262. Trips and Users**\n",
    "https://leetcode.com/problems/trips-and-users/description/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+-------+-------------------+----------+\n",
      "| id|client_id|driver_id|city_id|             status|request_at|\n",
      "+---+---------+---------+-------+-------------------+----------+\n",
      "|  1|        1|       10|      1|          completed|2013-10-01|\n",
      "|  2|        2|       11|      1|cancelled_by_driver|2013-10-01|\n",
      "|  3|        3|       12|      6|          completed|2013-10-01|\n",
      "|  4|        4|       13|      6|cancelled_by_client|2013-10-01|\n",
      "|  5|        1|       10|      1|          completed|2013-10-02|\n",
      "|  6|        2|       11|      6|          completed|2013-10-02|\n",
      "|  7|        3|       12|      6|          completed|2013-10-02|\n",
      "|  8|        2|       12|     12|          completed|2013-10-03|\n",
      "|  9|        3|       10|     12|          completed|2013-10-03|\n",
      "| 10|        4|       13|     12|cancelled_by_driver|2013-10-03|\n",
      "+---+---------+---------+-------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TaxiTrips\").getOrCreate()\n",
    "\n",
    "# Define Schema for 'Trips' Table\n",
    "trips_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"client_id\", IntegerType(), False),\n",
    "    StructField(\"driver_id\", IntegerType(), False),\n",
    "    StructField(\"city_id\", IntegerType(), False),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"request_at\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Sample Data for 'Trips' Table\n",
    "trips_data = [\n",
    "    (1, 1, 10, 1, \"completed\", \"2013-10-01\"),\n",
    "    (2, 2, 11, 1, \"cancelled_by_driver\", \"2013-10-01\"),\n",
    "    (3, 3, 12, 6, \"completed\", \"2013-10-01\"),\n",
    "    (4, 4, 13, 6, \"cancelled_by_client\", \"2013-10-01\"),\n",
    "    (5, 1, 10, 1, \"completed\", \"2013-10-02\"),\n",
    "    (6, 2, 11, 6, \"completed\", \"2013-10-02\"),\n",
    "    (7, 3, 12, 6, \"completed\", \"2013-10-02\"),\n",
    "    (8, 2, 12, 12, \"completed\", \"2013-10-03\"),\n",
    "    (9, 3, 10, 12, \"completed\", \"2013-10-03\"),\n",
    "    (10, 4, 13, 12, \"cancelled_by_driver\", \"2013-10-03\")\n",
    "]\n",
    "\n",
    "# Create DataFrame for 'Trips' table\n",
    "df_trips = spark.createDataFrame(trips_data, schema=trips_schema)\n",
    "\n",
    "# Register as SQL Table\n",
    "df_trips.createOrReplaceTempView(\"Trips\")\n",
    "\n",
    "# Show 'Trips' Table\n",
    "df_trips.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+\n",
      "|users_id|banned|  role|\n",
      "+--------+------+------+\n",
      "|       1|    No|client|\n",
      "|       2|   Yes|client|\n",
      "|       3|    No|client|\n",
      "|       4|    No|client|\n",
      "|      10|    No|driver|\n",
      "|      11|    No|driver|\n",
      "|      12|    No|driver|\n",
      "|      13|    No|driver|\n",
      "+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define Schema for 'Users' Table\n",
    "users_schema = StructType([\n",
    "    StructField(\"users_id\", IntegerType(), False),\n",
    "    StructField(\"banned\", StringType(), False),\n",
    "    StructField(\"role\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Sample Data for 'Users' Table\n",
    "users_data = [\n",
    "    (1, \"No\", \"client\"),\n",
    "    (2, \"Yes\", \"client\"),\n",
    "    (3, \"No\", \"client\"),\n",
    "    (4, \"No\", \"client\"),\n",
    "    (10, \"No\", \"driver\"),\n",
    "    (11, \"No\", \"driver\"),\n",
    "    (12, \"No\", \"driver\"),\n",
    "    (13, \"No\", \"driver\")\n",
    "]\n",
    "\n",
    "# Create DataFrame for 'Users' table\n",
    "df_users = spark.createDataFrame(users_data, schema=users_schema)\n",
    "\n",
    "# Register as SQL Table\n",
    "df_users.createOrReplaceTempView(\"Users\")\n",
    "\n",
    "# Show 'Users' Table\n",
    "df_users.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution 1 (longer 3 queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 98:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------------+---------+-------------+-------+-------------------+----------+\n",
      "| id|client_id|client_banned|driver_id|driver_banned|city_id|             status|request_at|\n",
      "+---+---------+-------------+---------+-------------+-------+-------------------+----------+\n",
      "|  1|        1|           No|       10|           No|      1|          completed|2013-10-01|\n",
      "|  3|        3|           No|       12|           No|      6|          completed|2013-10-01|\n",
      "|  4|        4|           No|       13|           No|      6|cancelled_by_client|2013-10-01|\n",
      "|  5|        1|           No|       10|           No|      1|          completed|2013-10-02|\n",
      "|  7|        3|           No|       12|           No|      6|          completed|2013-10-02|\n",
      "|  9|        3|           No|       10|           No|     12|          completed|2013-10-03|\n",
      "| 10|        4|           No|       13|           No|     12|cancelled_by_driver|2013-10-03|\n",
      "+---+---------+-------------+---------+-------------+-------+-------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW trips_no_banned_view AS\n",
    "select t.id, t.client_id, u.banned AS client_banned, t.driver_id, u2.banned as driver_banned, t.city_id, t.status, t.request_at\n",
    "from Trips t\n",
    "join Users u on t.client_id = u.users_id \n",
    "join Users u2 on t.driver_id = u2.users_id\n",
    "where u.banned = 'No' and u2.banned = 'No'\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query)\n",
    "spark.sql(\"SELECT * FROM trips_no_banned_view order by id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 135:==============>                                          (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------------+\n",
      "|request_at|             status|count_status|\n",
      "+----------+-------------------+------------+\n",
      "|2013-10-01|          completed|           2|\n",
      "|2013-10-01|cancelled_by_client|           1|\n",
      "|2013-10-02|          completed|           2|\n",
      "|2013-10-03|          completed|           1|\n",
      "|2013-10-03|cancelled_by_driver|           1|\n",
      "+----------+-------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query2 = \"\"\"\n",
    "create or replace temp view trips_groupby_status as\n",
    "select request_at, status, count(status) as count_status\n",
    "from trips_no_banned_view\n",
    "group by request_at, status\n",
    "order by request_at\"\"\"\n",
    "\n",
    "spark.sql(query2)\n",
    "\n",
    "spark.sql(\"select * from trips_groupby_status\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[MISSING_AGGREGATION] The non-aggregating expression \"status\" is based on columns which are not participating in the GROUP BY clause.\nAdd the columns or the expression to the GROUP BY, aggregate the expression, or use \"any_value(status)\" if you do not care which of the values within a group is returned.;\nAggregate [request_at#1177], [request_at#1177, (cast(CASE WHEN status#1178 IN (cancelled_by_client,cancelled_by_driver) THEN count_status#1179L ELSE cast(0 as bigint) END as double) / cast(sum(count_status#1179L) as double)) AS cancellation_rate#1175]\n+- SubqueryAlias trips_groupby_status\n   +- View (`trips_groupby_status`, [request_at#1177,status#1178,count_status#1179L])\n      +- Project [cast(request_at#1189 as string) AS request_at#1177, cast(status#1188 as string) AS status#1178, cast(count_status#1176L as bigint) AS count_status#1179L]\n         +- Sort [request_at#1189 ASC NULLS FIRST], true\n            +- Aggregate [request_at#1189, status#1188], [request_at#1189, status#1188, count(status#1188) AS count_status#1176L]\n               +- SubqueryAlias trips_no_banned_view\n                  +- View (`trips_no_banned_view`, [id#1182,client_id#1183,client_banned#1184,driver_id#1185,driver_banned#1186,city_id#1187,status#1188,request_at#1189])\n                     +- Project [cast(id#414 as int) AS id#1182, cast(client_id#415 as int) AS client_id#1183, cast(client_banned#1180 as string) AS client_banned#1184, cast(driver_id#416 as int) AS driver_id#1185, cast(driver_banned#1181 as string) AS driver_banned#1186, cast(city_id#417 as int) AS city_id#1187, cast(status#418 as string) AS status#1188, cast(request_at#419 as string) AS request_at#1189]\n                        +- Project [id#414, client_id#415, banned#452 AS client_banned#1180, driver_id#416, banned#1191 AS driver_banned#1181, city_id#417, status#418, request_at#419]\n                           +- Filter ((banned#452 = No) AND (banned#1191 = No))\n                              +- Join Inner, (driver_id#416 = users_id#1190)\n                                 :- Join Inner, (client_id#415 = users_id#451)\n                                 :  :- SubqueryAlias t\n                                 :  :  +- SubqueryAlias trips\n                                 :  :     +- View (`Trips`, [id#414,client_id#415,driver_id#416,city_id#417,status#418,request_at#419])\n                                 :  :        +- LogicalRDD [id#414, client_id#415, driver_id#416, city_id#417, status#418, request_at#419], false\n                                 :  +- SubqueryAlias u\n                                 :     +- SubqueryAlias users\n                                 :        +- View (`Users`, [users_id#451,banned#452,role#453])\n                                 :           +- LogicalRDD [users_id#451, banned#452, role#453], false\n                                 +- SubqueryAlias u2\n                                    +- SubqueryAlias users\n                                       +- View (`Users`, [users_id#1190,banned#1191,role#1192])\n                                          +- LogicalRDD [users_id#1190, banned#1191, role#1192], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m query3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mselect \u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    request_at, \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mgroup by request_at\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery3\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [MISSING_AGGREGATION] The non-aggregating expression \"status\" is based on columns which are not participating in the GROUP BY clause.\nAdd the columns or the expression to the GROUP BY, aggregate the expression, or use \"any_value(status)\" if you do not care which of the values within a group is returned.;\nAggregate [request_at#1177], [request_at#1177, (cast(CASE WHEN status#1178 IN (cancelled_by_client,cancelled_by_driver) THEN count_status#1179L ELSE cast(0 as bigint) END as double) / cast(sum(count_status#1179L) as double)) AS cancellation_rate#1175]\n+- SubqueryAlias trips_groupby_status\n   +- View (`trips_groupby_status`, [request_at#1177,status#1178,count_status#1179L])\n      +- Project [cast(request_at#1189 as string) AS request_at#1177, cast(status#1188 as string) AS status#1178, cast(count_status#1176L as bigint) AS count_status#1179L]\n         +- Sort [request_at#1189 ASC NULLS FIRST], true\n            +- Aggregate [request_at#1189, status#1188], [request_at#1189, status#1188, count(status#1188) AS count_status#1176L]\n               +- SubqueryAlias trips_no_banned_view\n                  +- View (`trips_no_banned_view`, [id#1182,client_id#1183,client_banned#1184,driver_id#1185,driver_banned#1186,city_id#1187,status#1188,request_at#1189])\n                     +- Project [cast(id#414 as int) AS id#1182, cast(client_id#415 as int) AS client_id#1183, cast(client_banned#1180 as string) AS client_banned#1184, cast(driver_id#416 as int) AS driver_id#1185, cast(driver_banned#1181 as string) AS driver_banned#1186, cast(city_id#417 as int) AS city_id#1187, cast(status#418 as string) AS status#1188, cast(request_at#419 as string) AS request_at#1189]\n                        +- Project [id#414, client_id#415, banned#452 AS client_banned#1180, driver_id#416, banned#1191 AS driver_banned#1181, city_id#417, status#418, request_at#419]\n                           +- Filter ((banned#452 = No) AND (banned#1191 = No))\n                              +- Join Inner, (driver_id#416 = users_id#1190)\n                                 :- Join Inner, (client_id#415 = users_id#451)\n                                 :  :- SubqueryAlias t\n                                 :  :  +- SubqueryAlias trips\n                                 :  :     +- View (`Trips`, [id#414,client_id#415,driver_id#416,city_id#417,status#418,request_at#419])\n                                 :  :        +- LogicalRDD [id#414, client_id#415, driver_id#416, city_id#417, status#418, request_at#419], false\n                                 :  +- SubqueryAlias u\n                                 :     +- SubqueryAlias users\n                                 :        +- View (`Users`, [users_id#451,banned#452,role#453])\n                                 :           +- LogicalRDD [users_id#451, banned#452, role#453], false\n                                 +- SubqueryAlias u2\n                                    +- SubqueryAlias users\n                                       +- View (`Users`, [users_id#1190,banned#1191,role#1192])\n                                          +- LogicalRDD [users_id#1190, banned#1191, role#1192], false\n"
     ]
    }
   ],
   "source": [
    "query3 = \"\"\"\n",
    "select \n",
    "    request_at, \n",
    "    SUM(CASE WHEN status IN ('cancelled_by_client', 'cancelled_by_driver') THEN count_status ELSE 0 END) / sum(count_status) AS cancellation_rate\n",
    "from trips_groupby_status\n",
    "group by request_at\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query3).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

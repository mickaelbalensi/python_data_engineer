{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow and ETL/ELT Concepts\n",
    "\n",
    "## Keywords\n",
    "\n",
    "### 1. ETL/ELT\n",
    "- **ETL (Extract, Transform, Load)**: Refers to the process of extracting data from different sources, transforming it into a desired format, and loading it into a destination.\n",
    "- **ELT (Extract, Load, Transform)**: A variant where data is first loaded into the destination before being transformed, often due to the power of modern data warehouses.\n",
    "\n",
    "### 2. DAG\n",
    "- **DAG (Directed Acyclic Graph)**: A graph structure used in Airflow to define workflows, where tasks are represented as nodes, and the edges represent dependencies between tasks. The key feature is that it does not allow cycles.\n",
    "\n",
    "### 3. Airflow Task\n",
    "- An **Airflow Task** is an individual unit of work in an Airflow DAG. Each task performs a specific operation and is represented as a node in the DAG.\n",
    "\n",
    "## Airflow Concepts\n",
    "\n",
    "### 1. with DAG\n",
    "- The `with DAG` context manager is used to define a set of tasks and their dependencies inside the DAG context. It simplifies the creation of the DAG.\n",
    "\n",
    "### 2. @dag / @task\n",
    "- `@dag` is used to define a DAG in Airflow (available from Airflow 2.0 onwards). It allows you to create a DAG using a decorator.\n",
    "- `@task` is a decorator used to define a task in Airflow, making it easier to write and manage tasks.\n",
    "\n",
    "### 3. >>, <<\n",
    "- These are used to set task dependencies in Airflow. \n",
    "    - `>>` sets downstream dependencies (i.e., task A >> task B means task A must run before task B).\n",
    "    - `<<` sets upstream dependencies (i.e., task A << task B means task B must run before task A).\n",
    "\n",
    "### 4. set_downstream, set_upstream\n",
    "- `set_downstream` is used to set downstream dependencies for a task.\n",
    "- `set_upstream` is used to set upstream dependencies for a task.\n",
    "\n",
    "### 5. cross_downstream\n",
    "- `cross_downstream` is a method to set downstream tasks across different branches in a DAG.\n",
    "\n",
    "### 6. chain\n",
    "- `chain` is a method to link a sequence of tasks, making them run in a specific order.\n",
    "\n",
    "---\n",
    "\n",
    "# Questions and Answers\n",
    "\n",
    "### 1. Why is Airflow better than a simple scheduler? When would you prefer the cron utility over Airflow?\n",
    "\n",
    "- **Airflow** `task dependencies`, retries, `logging`, and monitoring, `retring` when tasks fail\n",
    "    It supports `dynamic task creation`, execution logging, and better `error handling`, which cron doesn't.\n",
    "- **Cron** might be preferred for simpler, less complex scheduling needs where task dependencies, retries, and logs aren't required. For instance, when you need to run a basic script at a regular interval, cron can be sufficient.\n",
    "\n",
    "### 2. In what different ways can a task be triggered?\n",
    "\n",
    "- `Scheduled execution` based on DAG schedule.\n",
    "- `Manual execution` via the UI or CLI.\n",
    "- `Code changes` (if DAG definition changes).\n",
    "- `Programmatic execution` using trigger_dag or the Airflow REST API not from DAG but by external code\n",
    "- `Task dependencies` once they complete.\n",
    "\n",
    "### 3. Can Airflow run multiple tasks in parallel?\n",
    "\n",
    "- Yes, Airflow can run multiple tasks in parallel, provided that the necessary resources (e.g., worker capacity) are available. \n",
    "\n",
    "Airflow uses a distributed system of workers to run tasks in parallel.\n",
    "\n",
    "- `SequentialExecutor` which allows only one task to run at a time\n",
    "- `LocalExecutor`: Runs tasks in parallel on your `local machine` using multiple `processes`.\n",
    "- `CeleryExecutor`: Distributes tasks across `multiple worker nodes`, ideal for scaling horizontally.\n",
    "\n",
    "### 4. How can you handle dependencies between tasks?\n",
    "\n",
    "- Dependencies in Airflow are handled using:\n",
    "    - Task dependencies (`>>`, `<<`, `set_upstream`, `set_downstream`, `cross_downstream`).\n",
    "    - The `chain` method can also be used to define sequences of tasks.\n",
    "\n",
    "### 5. How can you monitor your workflow?\n",
    "\n",
    "- You can monitor your workflow in Airflow using:\n",
    "    - The `Airflow UI` to check DAG and task statuses.\n",
    "    - `Logs` generated by each task run.\n",
    "    - `Alerts` and `notifications` for failures and retries.\n",
    "    - Airflow's integration with `external tools` like `Grafana` or `Prometheus`.\n",
    "\n",
    "### 6. What are the two ways to create a DAG? Are there any pros and cons?\n",
    "\n",
    "- **1. Using the `DAG` context manager (`with DAG`):**\n",
    "    - **Pros**: Simplifies the definition of tasks and their dependencies, avoids code duplication, and provides clear DAG structure.\n",
    "    - **Cons**: Less flexibility for complex dynamic DAGs.\n",
    "- **2. Using the `@dag` decorator (Airflow 2.0+):**\n",
    "    - **Pros**: More Pythonic and concise, better readability, and simpler task management.\n",
    "    - **Cons**: Can be less intuitive for more complex DAG structures, and it may require a higher version of Airflow.\n",
    "- **3. Creating a DAG via the `Airflow UI`** \n",
    "    -  `not recommended for complex` workflows. It is mostly useful for `simple DAGs` or testing.\n",
    "\n",
    "### 7. Can a task have more than one upstream dependency?\n",
    "\n",
    "- Yes, a task can have multiple upstream dependencies in Airflow. This allows tasks to depend on the completion of several other tasks before they can run.\n",
    "\n",
    "# Set the dependencies\n",
    "`task1 >> final_task`\n",
    "`task2 >> final_task`\n",
    "`task3 >> final_task`\n",
    "\n",
    "final_task is the downstream task that depends on task1, task2, and task3\n",
    "\n",
    "### 8. How can you test a task individually if it has upstream dependencies?\n",
    "\n",
    "- You can test a task individually in Airflow by:\n",
    "    - Running the task using the `airflow tasks test` command, which allows you to skip upstream tasks.\n",
    "    - Triggering the task manually from the UI or CLI after ensuring that upstream tasks have been completed.\n",
    "\n",
    "### 9. What are the parameters to create a DAG?\n",
    "\n",
    "- Key parameters to create a DAG:\n",
    "    - `dag_id`: Unique identifier for the DAG.\n",
    "    - `default_args`: A dictionary of default parameters (e.g., retries, start date, etc.).\n",
    "    - `schedule_interval`: Defines the schedule (can be a cron expression or a timedelta).\n",
    "    - `catchup`: If True, Airflow will run past missed scheduled runs.\n",
    "    - `dagrun_timeout`: Maximum time allowed for a DAG to complete.\n",
    "    - `description`: A short description of the DAG.\n",
    "\n",
    "### 10. What are the possible states for a task?\n",
    "\n",
    "- The possible states for a task in Airflow are:\n",
    "    - `queued`: The task is waiting to be executed.\n",
    "    - `running`: The task is currently being executed.\n",
    "    - `success`: The task has successfully completed.\n",
    "    - `failed`: The task has failed during execution.\n",
    "    - `skipped`: The task was skipped due to conditional logic or other reasons.\n",
    "    - `up_for_retry`: The task has failed but is eligible for retrying.\n",
    "    - `up_for_reschedule`: The task is waiting to be rescheduled.\n",
    "\n",
    "### 11. Why is SLA important?\n",
    "\n",
    "- `SLA for a task`: You can set an SLA for each individual task to specify the `maximum time allowed for the task` to complete. This helps ensure that tasks finish in a timely manner.\n",
    "- `SLA Miss`: If a task takes longer than the defined SLA, an \"SLA miss\" is recorded. This can trigger notifications (such as emails) to alert the stakeholders.\n",
    "- SLA (Service Level Agreement) in Airflow is important because it sets the expectations for task completion time. It helps ensure that tasks are completed within the desired time frame, allows teams to track performance, and provides insight into the reliability and efficiency of workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

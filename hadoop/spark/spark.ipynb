{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark and Hadoop Comparison and Concepts\n",
    "\n",
    "## 1. What is the difference between Hadoop and Spark?\n",
    "- **Hadoop**:\n",
    "  - processing on `Disk` using MapReduce.\n",
    "  - `Batch processing` model. (No real time for data streaming)\n",
    "  - `Slower performance` due to frequent disk I/O operations.\n",
    "  - Typically uses `HDFS for storage` so fault tolerance leads to less memory efficient.\n",
    "  - Has it's `own data storage HDFS`\n",
    "  \n",
    "- **Spark**:\n",
    "  - `In-memory(RAM)` processing for faster performance. (faster 100)\n",
    "  - Supports `batch` and `real-time` processing `(Spark Streaming)`.\n",
    "  - Processes data using `RDDs, DataFrames, and Datasets`.\n",
    "  - `Fault-tolerant` and scales easily across clusters.\n",
    "\n",
    "    Spark's fault tolerance is achieved without needing data replication (like in Hadoop), which makes Spark `more memory-efficient`.\n",
    "  - spark doesn't have its own local storage. it use `external storage` like, HDFS, S3, mySQL etc\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why do we need Spark? Can't we just easily read files/databases directly?\n",
    "While itâ€™s possible to read files and databases directly, Spark is needed for:\n",
    "- **Scalability**: Can process large datasets across multiple nodes.\n",
    "- **Fault tolerance**: Data recovery in case of failures via lineage.\n",
    "- **Performance**: In-memory computation speeds up iterative algorithms.\n",
    "- **Distributed computing**: Efficient processing across a cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. What is a Spark Context?\n",
    "A **SparkContext** is the `entry point` for Spark applications. It connects the application to the cluster and facilitates the creation of RDDs, manages job execution, and provides access to Spark's capabilities for parallel processing. It's the main point of interaction with the underlying Spark infrastructure.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. What is the difference between a Session and a Context?\n",
    "- **SparkContext**: for `old` version Spark 1.x\n",
    "\n",
    "      managing functionality:\n",
    "      - like `job execution`, \n",
    "      - creating `RDDs`, \n",
    "      - and managing distributed resources.\n",
    "- **SparkSession**: for `new` version Spark 2.x\n",
    "      - A unified entry point for working with `structured data (DataFrames, SQL)`. \n",
    "      - It `includes SparkContext` and supports `higher-level APIs` such as DataFrame operations, SQL queries, and machine learning tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. What is the purpose of a Spark Cluster?\n",
    "A **Spark Cluster** is a collection of machines (nodes) working together to process large-scale data:\n",
    "- **Distributed storage**: Data is stored across multiple machines.\n",
    "- **Parallel processing**: Tasks are distributed across nodes for faster computation.\n",
    "- **Fault tolerance**: Provides high availability and data recovery in case of node failure.\n",
    "- **Scalability**: From a few nodes to thousands, allowing processing of massive datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. For each of the following modules/classes, explain what is its purpose and its advantages:**\n",
    "\n",
    "#### **RDD**: \n",
    "  - **Purpose**: \n",
    "    * A distributed collection of `data processed in parallel`.\n",
    "    * `unstrucured data`\n",
    "    * provide `low-level API` like map, reduce\n",
    "\n",
    "  - Advantages: `Fault tolerance`, `parallel processing`, and `immutability`\n",
    "\n",
    "#### **DataFrame and SQL**:\n",
    "  - Purpose: Structured data representation (like a table in a database). DataFrame provides a higher-level abstraction for working with distributed data.\n",
    "  - Advantages: Optimized query execution, supports SQL queries, and integrates with Sparkâ€™s ecosystem for machine learning and analytics.\n",
    "\n",
    "#### **Streaming**:\n",
    "  - Purpose: Real-time data processing (Spark Streaming).\n",
    "  - Advantages: Continuous data processing, easy integration with other systems, and support for complex event processing.\n",
    "\n",
    "#### **MLlib**:\n",
    "  - Purpose: Machine learning library for scalable algorithms (e.g., classification, regression).\n",
    "  - Advantages: Distributed machine learning, supports multiple algorithms, and integrates with Sparkâ€™s ecosystem.\n",
    "\n",
    "#### **GraphFrames**:\n",
    "  - Purpose: A library for working with graphs and performing graph processing.\n",
    "  - Advantages: Provides optimized graph algorithms and integrates seamlessly with DataFrames.\n",
    "\n",
    "#### **Resource**:\n",
    "  - Purpose: Manages and allocates resources (e.g., memory, CPU) in a Spark cluster.\n",
    "  - Advantages: Efficient resource utilization, fault tolerance, and scaling.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. What is the difference between a Spark DataFrame and a Pandas DataFrame?\n",
    "- **Spark DataFrame**:\n",
    "  - `Distributed` across a cluster, capable of handling large-scale data.\n",
    "  - `Lazy Execution`\n",
    "  - `SQL Integration`\n",
    "  - `Parallel Execution`\n",
    "  - `Real-time` Analytics, allow `data streaming`\n",
    "\n",
    "- **Pandas DataFrame**:\n",
    "  - `high-level` data manipulation functions for Python.\n",
    "  - `Index-Based` Operations: Uses row and column indexing\n",
    "\n",
    "---\n",
    "\n",
    "## 8. What are the Spark data sources?\n",
    "Spark supports various data sources:\n",
    "- **HDFS**: Hadoop Distributed File System.\n",
    "- **S3**: Amazonâ€™s cloud storage.\n",
    "- **JDBC**: For reading from relational databases.\n",
    "- **Parquet, Avro, ORC**: Columnar storage formats.\n",
    "- **JSON, CSV, Text**: File-based data sources.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. What is the difference between a transformation and an action?\n",
    "- **Transformation**:\n",
    "  - `Lazy operations` that return a new RDD/DataFrame (e.g., `map`, `filter`).\n",
    "\n",
    "- **Action**:\n",
    "  - `eager evaluation` compute the result (e.g., `collect`, `count`, `save`).\n",
    "\n",
    "---\n",
    "\n",
    "## 10. What are the advantages of laziness?\n",
    "- `Optimized execution`: combine all transformations to be performed as a single\n",
    "- `Fault tolerance`: spark will recompute lost partition from already loaded dataset\n",
    "- Spark only executes when necessary, `reducing memory` and CPU usage.\n",
    "---\n",
    "\n",
    "## 11. When is a shuffle operation needed?\n",
    "A **shuffle** occurs when:\n",
    "- `Data needs to be redistributed` across partitions.\n",
    "- This happens in operations like **groupBy**, **reduceByKey**, or **join**.\n",
    "- `Shuffling is expensive` and can impact performance due to data transfer and disk I/O.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Explain `explain`.\n",
    "explain() show:\n",
    "- `Physical plan` i.e. low lvl (sort, shuffle)\n",
    "- `Logical plan`  i.e. High lvl action (filter, groupby etc)\n",
    "- `Optimized Logical Plan` with rearrangement of action to\n",
    "  * **reduce data shuffling**\n",
    "  * **pushing down filter and projection**\n",
    "  * **Choosing efficient join strategies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|department|avg_salary|\n",
      "+----------+----------+\n",
      "|        IT|    2750.0|\n",
      "|   Finance|    2700.0|\n",
      "+----------+----------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Sort ['avg_salary DESC NULLS LAST], true\n",
      "+- Aggregate [department#14], [department#14, avg(salary#15L) AS avg_salary#25]\n",
      "   +- Filter (salary#15L > cast(2200 as bigint))\n",
      "      +- LogicalRDD [id#12L, name#13, department#14, salary#15L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "department: string, avg_salary: double\n",
      "Sort [avg_salary#25 DESC NULLS LAST], true\n",
      "+- Aggregate [department#14], [department#14, avg(salary#15L) AS avg_salary#25]\n",
      "   +- Filter (salary#15L > cast(2200 as bigint))\n",
      "      +- LogicalRDD [id#12L, name#13, department#14, salary#15L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [avg_salary#25 DESC NULLS LAST], true\n",
      "+- Aggregate [department#14], [department#14, avg(salary#15L) AS avg_salary#25]\n",
      "   +- Project [department#14, salary#15L]\n",
      "      +- Filter (isnotnull(salary#15L) AND (salary#15L > 2200))\n",
      "         +- LogicalRDD [id#12L, name#13, department#14, salary#15L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [avg_salary#25 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(avg_salary#25 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=101]\n",
      "      +- HashAggregate(keys=[department#14], functions=[avg(salary#15L)], output=[department#14, avg_salary#25])\n",
      "         +- Exchange hashpartitioning(department#14, 200), ENSURE_REQUIREMENTS, [plan_id=98]\n",
      "            +- HashAggregate(keys=[department#14], functions=[partial_avg(salary#15L)], output=[department#14, sum#36, count#37L])\n",
      "               +- Project [department#14, salary#15L]\n",
      "                  +- Filter (isnotnull(salary#15L) AND (salary#15L > 2200))\n",
      "                     +- Scan ExistingRDD[id#12L,name#13,department#14,salary#15L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, desc\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"ExplainExample\").getOrCreate()\n",
    "\n",
    "# Create DataFrame\n",
    "data = [\n",
    "    (1, \"Alice\", \"HR\", 2000),\n",
    "    (2, \"Bob\", \"IT\", 2500),\n",
    "    (3, \"Charlie\", \"IT\", 3000),\n",
    "    (4, \"David\", \"HR\", 2200),\n",
    "    (5, \"Eve\", \"Finance\", 2700),\n",
    "]\n",
    "columns = [\"id\", \"name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# ðŸ”¹ Transformations (Lazy)\n",
    "df_filtered = df.filter(col(\"salary\") > 2200)  # Filter rows\n",
    "df_grouped = df_filtered.groupBy(\"department\").agg(avg(\"salary\").alias(\"avg_salary\"))  # Aggregate\n",
    "df_sorted = df_grouped.orderBy(desc(\"avg_salary\"))  # Sort results\n",
    "\n",
    "# ðŸ”¹ Action (Triggers execution)\n",
    "df_sorted.show()\n",
    "\n",
    "df_sorted.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. What is the importance of repartition?\n",
    "Repartitioning redistributes data across partitions, which can:\n",
    "- **Improve performance**: By balancing the data across nodes, reducing skew.\n",
    "- **Optimize parallelism**: Ensures a better workload distribution.\n",
    "- **Avoid shuffling**: Proper partitioning can prevent unnecessary shuffling and improve job performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 14. Describe a use case for map and another for mapPartitions.\n",
    "- **map**: Use `map` when you need to apply a function to each element in an RDD/DataFrame independently. Example: Squaring each number in a dataset.\n",
    "- **mapPartitions**: Use `mapPartitions` when you want to apply a function to each partition of data. Itâ€™s more efficient for operations that require access to the entire partition (e.g., accessing a database connection per partition).\n",
    "\n",
    "---\n",
    "\n",
    "## 15. Is there a parallel for SQL constraints in Spark? What about indexes? If yes - what is it? If no - why?\n",
    "- **SQL Constraints**: Spark SQL does not support SQL constraints like `PRIMARY KEY` or `FOREIGN KEY`. It focuses on distributed processing and does not enforce data integrity constraints.\n",
    "- **Indexes**: Spark does not support traditional indexes like in relational databases. However, performance can be optimized with partitioning, bucketing, and using the appropriate storage formats (e.g., Parquet).\n",
    "\n",
    "---\n",
    "\n",
    "## 16. Why and when are `lit` and `col` useful?\n",
    "- **lit**: Used to create a column of constant values (e.g., `lit(10)` to create a column with the value `10`).\n",
    "- **col**: Used to refer to a column in DataFrame operations (e.g., `col('age')` to reference the \"age\" column).\n",
    "- These functions are useful in DataFrame transformations and operations.\n",
    "\n",
    "---\n",
    "\n",
    "## 17. What is the difference between parquet files and CSV files?\n",
    "- **Parquet**:\n",
    "  - Columnar format, optimized for storage and querying.\n",
    "  - Supports compression and faster read/writes.\n",
    "  - Better for big data due to its efficient storage and performance.\n",
    "\n",
    "- **CSV**:\n",
    "  - Row-based text format, simple but inefficient for large datasets.\n",
    "  - No schema, slower read/writes, and lacks compression.\n",
    "\n",
    "---\n",
    "\n",
    "## 18. Can we read data directly from a JSON file using Spark? How? Why would we do that?\n",
    "Yes, Spark can read data from a JSON file using the `spark.read.json()` method:\n",
    "```python\n",
    "df = spark.read.json(\"path_to_json_file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark and Hadoop Comparison and Concepts\n",
    "\n",
    "## 1. What is the difference between Hadoop and Spark?\n",
    "- **Hadoop**:\n",
    "  - processing on `Disk` using MapReduce.\n",
    "  - `Batch processing` model. (No real time for data streaming)\n",
    "  - `Slower performance` due to frequent disk I/O operations.\n",
    "  - Typically uses `HDFS for storage` so fault tolerance leads to less memory efficient.\n",
    "  - Has it's `own data storage HDFS`\n",
    "  \n",
    "- **Spark**:\n",
    "  - `In-memory(RAM)` processing for faster performance. (faster 100)\n",
    "  - Supports `batch` and `real-time` processing `(Spark Streaming)`.\n",
    "  - Processes data using `RDDs, DataFrames, and Datasets`.\n",
    "  - `Fault-tolerant` and scales easily across clusters.\n",
    "\n",
    "    Spark's fault tolerance is achieved without needing data replication (like in Hadoop), which makes Spark `more memory-efficient`.\n",
    "  - spark doesn't have its own local storage. it use `external storage` like, HDFS, S3, mySQL etc\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why do we need Spark? Can't we just easily read files/databases directly?\n",
    "While it’s possible to read files and databases directly, Spark is needed for:\n",
    "- **Scalability**: Can process large datasets across multiple nodes.\n",
    "- **Fault tolerance**: Data recovery in case of failures via lineage.\n",
    "- **Performance**: In-memory computation speeds up iterative algorithms.\n",
    "- **Distributed computing**: Efficient processing across a cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. What is a Spark Context?\n",
    "A **SparkContext** is the `entry point` for Spark applications. It connects the application to the cluster and facilitates the creation of RDDs, manages job execution, and provides access to Spark's capabilities for parallel processing. It's the main point of interaction with the underlying Spark infrastructure.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. What is the difference between a Session and a Context?\n",
    "- **SparkContext**: for `old` version Spark 1.x\n",
    "\n",
    "      managing functionality:\n",
    "      - like `job execution`, \n",
    "      - creating `RDDs`, \n",
    "      - and managing distributed resources.\n",
    "- **SparkSession**: for `new` version Spark 2.x\n",
    "      - A unified entry point for working with `structured data (DataFrames, SQL)`. \n",
    "      - It `includes SparkContext` and supports `higher-level APIs` such as DataFrame operations, SQL queries, and machine learning tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. What is the purpose of a Spark Cluster?\n",
    "A **Spark Cluster** is a collection of machines (nodes) working together to process large-scale data:\n",
    "- **Distributed storage**: Data is stored across multiple machines.\n",
    "- **Parallel processing**: Tasks are distributed across nodes for faster computation.\n",
    "- **Fault tolerance**: Provides high availability and data recovery in case of node failure.\n",
    "- **Scalability**: From a few nodes to thousands, allowing processing of massive datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. For each of the following modules/classes, explain what is its purpose and its advantages:**\n",
    "\n",
    "#### **RDD**: \n",
    "  - **Purpose**: \n",
    "    * A distributed collection of `data processed in parallel`.\n",
    "    * `unstrucured data`\n",
    "    * provide `low-level API` like map, reduce\n",
    "\n",
    "  - Advantages: `Fault tolerance`, `parallel processing`, and `immutability`\n",
    "\n",
    "#### **DataFrame and SQL**:\n",
    "  - Purpose: Structured data representation (like a table in a database). DataFrame provides a higher-level abstraction for working with distributed data.\n",
    "  - Advantages: Optimized query execution, supports SQL queries, and integrates with Spark’s ecosystem for machine learning and analytics.\n",
    "\n",
    "#### **Streaming**:\n",
    "  - Purpose: Real-time data processing (Spark Streaming).\n",
    "  - Advantages: Continuous data processing, easy integration with other systems, and support for complex event processing.\n",
    "\n",
    "#### **MLlib**:\n",
    "  - Purpose: Machine learning library for scalable algorithms (e.g., classification, regression).\n",
    "  - Advantages: Distributed machine learning, supports multiple algorithms, and integrates with Spark’s ecosystem.\n",
    "\n",
    "#### **GraphFrames**:\n",
    "  - Purpose: A library for working with graphs and performing graph processing.\n",
    "  - Advantages: Provides optimized graph algorithms and integrates seamlessly with DataFrames.\n",
    "\n",
    "#### **Resource**:\n",
    "  - Purpose: Manages and allocates resources (e.g., memory, CPU) in a Spark cluster.\n",
    "  - Advantages: Efficient resource utilization, fault tolerance, and scaling.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. What is the difference between a Spark DataFrame and a Pandas DataFrame?\n",
    "- **Spark DataFrame**:\n",
    "  - Distributed across a cluster, capable of handling large-scale data.\n",
    "  - Optimized for distributed computations and fault tolerance.\n",
    "  - Supports operations on distributed data with APIs in Spark.\n",
    "\n",
    "- **Pandas DataFrame**:\n",
    "  - Stored in memory on a single machine.\n",
    "  - Best for small datasets that can fit into the machine's memory.\n",
    "  - Provides high-level data manipulation functions for Python.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. What are the Spark data sources?\n",
    "Spark supports various data sources:\n",
    "- **HDFS**: Hadoop Distributed File System.\n",
    "- **S3**: Amazon’s cloud storage.\n",
    "- **JDBC**: For reading from relational databases.\n",
    "- **Parquet, Avro, ORC**: Columnar storage formats.\n",
    "- **JSON, CSV, Text**: File-based data sources.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. What is the difference between a transformation and an action?\n",
    "- **Transformation**:\n",
    "  - Lazy operations that return a new RDD/DataFrame (e.g., `map`, `filter`).\n",
    "  - They are not executed immediately; Spark builds a DAG (Directed Acyclic Graph) and only executes when an action is called.\n",
    "\n",
    "- **Action**:\n",
    "  - Triggers execution of the transformations to compute the result (e.g., `collect`, `count`, `save`).\n",
    "  - Forces Spark to perform the actual computation.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. What are the advantages of laziness?\n",
    "- **Optimized execution**: Spark optimizes the entire DAG of operations before running them.\n",
    "- **Reduced unnecessary computation**: Only the necessary transformations are computed when an action is triggered.\n",
    "- **Improved performance**: Avoids intermediate computations and minimizes data shuffling.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. When is a shuffle operation needed?\n",
    "A **shuffle** occurs when:\n",
    "- Data needs to be redistributed across partitions.\n",
    "- This happens in operations like **groupBy**, **reduceByKey**, or **join**.\n",
    "- Shuffling is expensive and can impact performance due to data transfer and disk I/O.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Explain `explain`.\n",
    "`explain()` is a method in Spark that outputs the logical and physical plan of a DataFrame or RDD. It shows how Spark plans to execute the computation, including stages, tasks, and optimizations (like predicate pushdown). It helps with performance tuning by understanding query execution.\n",
    "\n",
    "---\n",
    "\n",
    "## 13. What is the importance of repartition?\n",
    "Repartitioning redistributes data across partitions, which can:\n",
    "- **Improve performance**: By balancing the data across nodes, reducing skew.\n",
    "- **Optimize parallelism**: Ensures a better workload distribution.\n",
    "- **Avoid shuffling**: Proper partitioning can prevent unnecessary shuffling and improve job performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 14. Describe a use case for map and another for mapPartitions.\n",
    "- **map**: Use `map` when you need to apply a function to each element in an RDD/DataFrame independently. Example: Squaring each number in a dataset.\n",
    "- **mapPartitions**: Use `mapPartitions` when you want to apply a function to each partition of data. It’s more efficient for operations that require access to the entire partition (e.g., accessing a database connection per partition).\n",
    "\n",
    "---\n",
    "\n",
    "## 15. Is there a parallel for SQL constraints in Spark? What about indexes? If yes - what is it? If no - why?\n",
    "- **SQL Constraints**: Spark SQL does not support SQL constraints like `PRIMARY KEY` or `FOREIGN KEY`. It focuses on distributed processing and does not enforce data integrity constraints.\n",
    "- **Indexes**: Spark does not support traditional indexes like in relational databases. However, performance can be optimized with partitioning, bucketing, and using the appropriate storage formats (e.g., Parquet).\n",
    "\n",
    "---\n",
    "\n",
    "## 16. Why and when are `lit` and `col` useful?\n",
    "- **lit**: Used to create a column of constant values (e.g., `lit(10)` to create a column with the value `10`).\n",
    "- **col**: Used to refer to a column in DataFrame operations (e.g., `col('age')` to reference the \"age\" column).\n",
    "- These functions are useful in DataFrame transformations and operations.\n",
    "\n",
    "---\n",
    "\n",
    "## 17. What is the difference between parquet files and CSV files?\n",
    "- **Parquet**:\n",
    "  - Columnar format, optimized for storage and querying.\n",
    "  - Supports compression and faster read/writes.\n",
    "  - Better for big data due to its efficient storage and performance.\n",
    "\n",
    "- **CSV**:\n",
    "  - Row-based text format, simple but inefficient for large datasets.\n",
    "  - No schema, slower read/writes, and lacks compression.\n",
    "\n",
    "---\n",
    "\n",
    "## 18. Can we read data directly from a JSON file using Spark? How? Why would we do that?\n",
    "Yes, Spark can read data from a JSON file using the `spark.read.json()` method:\n",
    "```python\n",
    "df = spark.read.json(\"path_to_json_file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
